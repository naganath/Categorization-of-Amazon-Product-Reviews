# Categorization-of-Amazon-Product-Reviews-
Aspect based sentiment analysis was done on the customer reviews from Amazon to categorize them based on the areas of the company     such as Packaging, Logistics, Quality, Specifications and Other operations in order to find the low performing areas of the company. 5-core     dataset of amazon reviews prepared by University of California was considered. Different deep learning models such as RNN, CNN and LSTM        were employed and compared to come up with best accuracy. GloVe model was used as pre-trained word embedding for the models.

Product reviews are an essential part of an online store’s branding and marketing. Analyzing these reviews helps the business to read between the lines and hone in on their sales. With increasing popularity for extracting information from the reviews, different perspectives and techniques are constantly attempted to gain intelligence as much as possible. Existing work usually assumes that the information gathered is focused on personalizing the services and improving product quality. Apart from these, the reviews also have valuable insights into the company’s structure and its performances. In this project, we propose a different categorization of amazon reviews that are going to reveal the additional reasons for customer’s bad reviews and the direct dependencies on the company’s internal structure. The key idea is to filter out the bad reviews and associate the cause of it with the internal organization of the company. Results from the experiment on different product categories gave some awareness of the need to improve the sub- org of the company.

# Data and Preprocessing
The Amazon review dataset was published in 2018 by University of California, San Diego. The total number of reviews (raw review data) is 233.1 million and it includes reviews from May 1996 to Oct 2018 across all categories. For the experiment, 5-core dataset was used i.e the subset of the data in which all the users and the
items have at least 5 reviews associated with them and that constitutes about 75.26M reviews. The data is currently available in json format as one review per line. The 5-core dataset was huge and contained lots of badly written user reviews. The dataset also contained attributes that are irrelevant to our experiment like description, price, image url and so on. The review text, summary and product category from the data was used for the experiment. Firstly, for each product category, we selected reviews with overall ratings less than or equal to 2 because the goal of the project is to identify the low performing areas. The next step as part of preprocessing is to tag the filtered reviews into different classes based on the review content. The five different classes that were used for the experiment are Quality, Specifications, Logistics, Operations and Others. Each product category has its own set of keywords to be tagged into an output class. So for each chosen category, as far as the training data is concerned, we did manual tagging for 200 reviews to find the set of keywords for each output class and then we automated the tagging to tag other reviews. Again, for test data, we took 200 reviews from each chosen category and manually tagged them to their respective output classes. This is how we have generated out test and train data for the experiment.The training dataset has approximately 16.5k reviews and the testing dataset has approximately 1.2k reviews across all the output classes.

# Conclusion
 In this project, we have proposed a way to categorize the Amazon product reviews from a different perspective. As we have already mentioned, customer reviews under any platform contain a lot of information that can be inferred. Hence a different perspective of reviews are necessary to gather more nuances of customers feeling. In this project, we have shown that LSTM/GRU networks have been successful in classifying reviews based on the services provided by amazon. We also proved that a simple RNN is not sufficient for this classification. Some of the possible future direction is to multi-tag each review to better prepare the dataset. For eg, reviews like “item arrived late and package was opened when it arrived” can be either categorized in Logistics and Operations. Now, our experiment focuses on only categorizing the reviews into one class. Multi-tagging the reviews can help in bringing more insights and better predict the low performing areas.
